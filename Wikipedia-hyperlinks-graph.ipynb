{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia hyperlinks Graph\n",
    "### Algorithmic Methods for Data Mining - Homework 5\n",
    "##### Group #17 - Guilherme Nicchio, Joanna Broniarek, Melis\n",
    "##### 23/12/2018\n",
    "\n",
    "Introduction\n",
    "======\n",
    "\n",
    "The goal of this project consists in perform an analysis of the Wikipedia Hyperlink graph. In particular, given extra information about the categories to which an article belongs to we rank the articles according to some criteria.\n",
    "\n",
    "For this purpose we use the Wikipedia graph released by the SNAP group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T10:05:26.138511Z",
     "start_time": "2018-12-23T10:05:26.133524Z"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pickle\n",
    "import collections\n",
    "from tqdm import tqdm, tnrange, tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [RQ1]\n",
    "**Creating Graph**\n",
    "\n",
    "Build the graph G=(V, E), where V is the set of articles and E the hyperlinks among them, and provide its basic information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T09:54:55.229279Z",
     "start_time": "2018-12-23T09:54:55.225314Z"
    }
   },
   "outputs": [],
   "source": [
    "DG = nx.DiGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T09:54:56.389521Z",
     "start_time": "2018-12-23T09:54:56.386494Z"
    }
   },
   "outputs": [],
   "source": [
    "path = \"C:/Users/guilh/Desktop/ADM/HW5/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Wikicat hyperlink graph. A reduced version of the one you find on SNAP. Every row is an edge, the two elements are the nodes (source and destination)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T09:55:11.013745Z",
     "start_time": "2018-12-23T09:54:56.854745Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(path + \"wiki-topcats-reduced.txt\", \"r\") as f:\n",
    "    #create graph\n",
    "    for line in f.readlines():\n",
    "        article1, article2 = line.split()\n",
    "        DG.add_weighted_edges_from([(int(article1), int(article2), 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T09:55:11.470827Z",
     "start_time": "2018-12-23T09:55:11.100819Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is graph directed?  True \n",
      "\n",
      "Since the neighbors of 52nd node are: 401135 1069112 1163551 ,52nd node is not neighbor for 1069112nd node which has connection with 1060396 1061304 1062611 1066969 1069008 1069113 1069258 1069275 1656982  nodes. \n",
      "\n",
      "The number of nodes: 461193 \n",
      "\n",
      "The number of edges: 2645247\n"
     ]
    }
   ],
   "source": [
    "# Is graph directed\n",
    "print(\"Is graph directed? \", nx.is_directed(DG), \"\\n\") # check whether graph is directed or not\n",
    "\n",
    "# example;\n",
    "print(\"Since the neighbors of 52nd node are:\", *list(DG.neighbors(52)),\n",
    "      \",52nd node is not neighbor for 1069112nd node which has connection with\", *list(DG.neighbors(1069112)),\" nodes. \\n\")\n",
    "\n",
    "# The number of nodes\n",
    "nodes_num = DG.number_of_nodes()\n",
    "print(\"The number of nodes:\", nodes_num, \"\\n\") # also len(DG) works\n",
    "\n",
    "# The number of edges\n",
    "edges_num = DG.number_of_edges()\n",
    "print(\"The number of edges:\", edges_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Graph density**\n",
    "In mathematics, a dense graph is a graph in which the number of edges is close to the maximal number of edges. The opposite, a graph with only a few edges, is a sparse graph. The distinction between sparse and dense graphs is rather vague, and depends on the context.\n",
    "\n",
    "For directed graphs, the graph density is defined as:\n",
    "$$D = \\frac{|E|}{|V|(|V|-1)}$$\n",
    "\n",
    "where E is the number of edges and V is the number of vertices in the graph. The maximum number of edges for an directed graph is $|V|(|V|-1).$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T09:55:17.099151Z",
     "start_time": "2018-12-23T09:55:17.093166Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.590986317548208e-08\n"
     ]
    }
   ],
   "source": [
    "density = nodes_num / (edges_num*(edges_num-1))\n",
    "print(density);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [RQ2] \n",
    "Given a category $C_0 = \\{article_1, article_2, \\dots \\}$ as input we want to rank all of the nodes in V according to the block-ranking, where the blocks are represented by the categories:\n",
    "$$block_{RANKING} =\\begin{bmatrix} C_0 \\\\ C_1 \\\\ \\dots \\\\ C_c\\\\ \\end{bmatrix}$$\n",
    "\n",
    "Each category  corresponds to a list of nodes.\n",
    "\n",
    "The first category of the rank, $C_0$, always corresponds to the input category. The order of the remaining categories is given by:\n",
    "\n",
    "$$distance(C_0, C_i) = median(ShortestPath(C_0, C_i))$$\n",
    "\n",
    "The lower is the distance from $C_0$, the higher is the $C_i$ position in the rank. $ShortestPath(C_0, C_i)$ is the set of all the possible shortest paths between the nodes of $C_0$ and $C_i$. Moreover, the length of a path is given by the sum of the weights of the edges it is composed by."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reading the file with categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T09:55:26.292565Z",
     "start_time": "2018-12-23T09:55:24.789056Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(path + \"wiki-topcats-categories.txt\", \"r\") as f2:\n",
    "    categories = {} # {category0 : [article1, article2, ...], ...., 5: [23, 45, 6]}\n",
    "    categories_names = {} # {category_name : index, ...}\n",
    "    categories_names_by_index = {}\n",
    "    for cat_indx, line in enumerate(f2.readlines()):\n",
    "        line_content = line.split(\";\")\n",
    "        categories[cat_indx] = list(map(int, line_content[1].split()))\n",
    "        categories_names[line_content[0].split(\":\")[1]] = cat_indx\n",
    "        categories_names_by_index[cat_indx] = line_content[0].split(\":\")[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide the name of category $C0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T09:59:16.140754Z",
     "start_time": "2018-12-23T09:59:03.952851Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please, choose the name of category: \n",
      "\n",
      "Fellows_of_the_Royal_Society\n",
      "The index of selected category:  10839\n"
     ]
    }
   ],
   "source": [
    "C0_name = input(\"Please, choose the name of category: \\n\\n\")\n",
    "C0_idx = categories_names[C0_name]\n",
    "print(\"The index of selected category: \", C0_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filtering categories which exist in our reduced graph:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T10:01:36.948192Z",
     "start_time": "2018-12-23T10:01:36.933204Z"
    }
   },
   "outputs": [],
   "source": [
    "tmp_selected_category_indx = []\n",
    "# filtering categories with nodes more than 3500\n",
    "for i in range(len(categories)):\n",
    "    if len(categories[i]) > 3500:\n",
    "        tmp_selected_category_indx.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing categories which exist in our reduced graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T10:01:42.305078Z",
     "start_time": "2018-12-23T10:01:42.298069Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "461193"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(DG.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T10:01:48.763847Z",
     "start_time": "2018-12-23T10:01:47.530001Z"
    }
   },
   "outputs": [],
   "source": [
    "grouped_categories_nodes = [] # nodes grouped per category -- without C0 category\n",
    "categories_nodes = set() # all nodes together -- without C0 category\n",
    "\n",
    "# chose the category C0 with nodes only included in the DG graph:\n",
    "C0 = set(categories[C0_idx]).intersection(DG.nodes)\n",
    "\n",
    "final_selected_category_indx = []\n",
    "# chose categories with nodes only included in the DG graph:\n",
    "for idx in tmp_selected_category_indx[1:]:\n",
    "    tmp_categ = set(categories[idx]).intersection(DG.nodes)\n",
    "    # if C_i contains different nodes than C0:\n",
    "    C_i = tmp_categ - C0\n",
    "    if len(C_i) != 0 and len(C_i) < 100000:\n",
    "        final_selected_category_indx.append(idx)\n",
    "        grouped_categories_nodes.append(C_i)\n",
    "        categories_nodes = categories_nodes.union(C_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T11:03:29.073793Z",
     "start_time": "2018-12-23T11:03:29.067782Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(grouped_categories_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T10:02:00.805962Z",
     "start_time": "2018-12-23T10:02:00.798982Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3446"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(C0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BFS Shortest path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breadth-first search (BFS) is an algorithm for traversing or searching tree or graph data structures. It starts at the tree root (or some arbitrary node of a graph, sometimes referred to as a 'search key'), and explores all of the neighbor nodes at the present depth prior to moving on to the nodes at the next depth level.\n",
    "\n",
    "It uses the opposite strategy as depth-first search, which instead explores the highest-depth nodes first before being forced to backtrack and expand shallower nodes.\n",
    "\n",
    "BFS and its application in finding connected components of graphs were invented in 1945 by Konrad Zuse, in his (rejected) Ph.D. thesis on the Plankalkül programming language, but this was not published until 1972. It was reinvented in 1959 by Edward F. Moore, who used it to find the shortest path out of a maze, and later developed by C. Y. Lee into a wire routing algorithm (published 1961). Source: Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-22T12:42:24.579009Z",
     "start_time": "2018-12-22T12:42:24.571032Z"
    }
   },
   "outputs": [],
   "source": [
    "def bfs_shortest_path(graph, start, categories_nodes):\n",
    "    visited_dict = defaultdict(lambda:[False])\n",
    "    queue = [start]\n",
    "    visited_dict[start] = 0\n",
    "    \n",
    "    while queue:\n",
    "        node = queue.pop(0)\n",
    "        distance = visited_dict[node]\n",
    "        try:\n",
    "            for neighbour in graph.neighbors(node):\n",
    "                if visited_dict[neighbour]==[False]:\n",
    "                    visited_dict[neighbour] = distance + 1\n",
    "                    queue.append(neighbour)\n",
    "        except KeyError: pass\n",
    "    return {node:visited_dict[node] for node in categories_nodes}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this algorithm we can go trhough each node of the input category and compute the distances between it and all the other nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_distances = {}\n",
    "counter = 0\n",
    "\n",
    "for idx, node in enumerate(C0):\n",
    "    article_distances[node] = bfs_shortest_path(DG, node, categories_nodes)\n",
    "    if (idx+1)%100==0:\n",
    "        with open('distance_' + str(counter) + '.pkl', 'wb') as file:\n",
    "            pickle.dump(article_distances, file, pickle.HIGHEST_PROTOCOL)\n",
    "        article_distances = dict(); counter+=1\n",
    "\n",
    "with open('distance_' + str(counter) + '.pkl', 'wb') as file:\n",
    "    pickle.dump(article_distances, file, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping categories with distances computed in the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-22T21:39:28.210605Z",
     "start_time": "2018-12-22T21:39:28.205618Z"
    }
   },
   "outputs": [],
   "source": [
    "path = \"C:/Users/guilh/Desktop/ADM/HW5/distances_files/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T03:44:46.873262Z",
     "start_time": "2018-12-22T21:39:52.446759Z"
    }
   },
   "outputs": [],
   "source": [
    "distances_categories = {}\n",
    "    \n",
    "#for each category selected previously excluding C0\n",
    "for i in tqdm(final_selected_category_indx):\n",
    "    distances = []\n",
    "    #for each node in this selected category underanalysis\n",
    "    for j in tnrange(35):\n",
    "        with open(path + 'distance_' + str(j) + '.pkl', 'rb') as file:\n",
    "            distance_dict = pickle.load(file)\n",
    "            for node in categories[i]:\n",
    "                #for the starting node of C0 into our distance file\n",
    "                for starting_node in distance_dict:\n",
    "                    #try to find the distances from C0 node to the node inside the category under analysis\n",
    "                    try:\n",
    "                        d = distance_dict[starting_node][node]\n",
    "                        if d != [False]:\n",
    "                            distances.append(d)\n",
    "                            #if distance is false append 9999\n",
    "                        else: distances.append(9999)\n",
    "\n",
    "                    except: pass\n",
    "            #append the results of distances to this category\n",
    "    distances = np.array(distances)\n",
    "    median = np.median(distances)\n",
    "    distances_categories[i] = median\n",
    "    \n",
    "    print(i, median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T10:03:31.858912Z",
     "start_time": "2018-12-23T10:03:31.845945Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('median_distances' + '.pkl', 'wb') as file:\n",
    "    pickle.dump(distances_categories, file, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the saved file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T10:03:39.008302Z",
     "start_time": "2018-12-23T10:03:39.002317Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('median_distances' + '.pkl', 'rb') as file:\n",
    "    distances_median = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T10:03:56.113354Z",
     "start_time": "2018-12-23T10:03:56.109361Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted_by_value = sorted(distances_median.items(), key=lambda kv: kv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T10:04:02.813432Z",
     "start_time": "2018-12-23T10:04:02.579058Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input category: Fellows_of_the_Royal_Society\n",
      "Distances:\n",
      "Members_of_the_United_Kingdom_Parliament_for_English_constituencies - 6.0\n",
      "English_television_actors - 6.0\n",
      "British_films - 6.0\n",
      "English-language_films - 6.0\n",
      "American_films - 6.0\n",
      "People_from_New_York_City - 6.0\n",
      "American_Jews - 6.0\n",
      "American_television_actors - 6.0\n",
      "American_film_actors - 6.0\n",
      "Black-and-white_films - 6.0\n",
      "Article_Feedback_Pilot - 6.0\n",
      "Harvard_University_alumni - 7.0\n",
      "Indian_films - 7.0\n",
      "Rivers_of_Romania - 7.0\n",
      "English-language_albums - 7.0\n",
      "Debut_albums - 7.0\n",
      "American_military_personnel_of_World_War_II - 7.0\n",
      "The_Football_League_players - 8.0\n",
      "Major_League_Baseball_pitchers - 8.0\n",
      "Year_of_birth_missing_(living_people) - 8.0\n",
      "Place_of_birth_missing_(living_people) - 8.0\n",
      "Windows_games - 8.0\n",
      "English_cricketers - 9.0\n",
      "Association_football_forwards - 10.0\n",
      "Association_football_goalkeepers - 9999.0\n",
      "Association_football_midfielders - 9999.0\n",
      "Association_football_defenders - 9999.0\n",
      "Year_of_birth_unknown - 9999.0\n",
      "Year_of_death_missing - 9999.0\n",
      "Main_Belt_asteroids - 9999.0\n",
      "Asteroids_named_for_people - 9999.0\n",
      "Year_of_birth_missing - 9999.0\n"
     ]
    }
   ],
   "source": [
    "print('Input category:', categories_names_by_index[10839])\n",
    "print('Distances:')\n",
    "for i in sorted_by_value:\n",
    "    print(categories_names_by_index[i[0]], '-', i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is reasonable that the categories with most connections with \"Fellows of the Royal Society\" are categories related to famous people and public figures of United Kingdom as politicals and actors while the less relevant are in the end of the rank with weights as 9999, which mean that in this categories must articles don't have a connection to the input category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T10:04:11.296246Z",
     "start_time": "2018-12-23T10:04:11.292257Z"
    }
   },
   "outputs": [],
   "source": [
    "sorted_categories_idx = [i[0] for i in sorted_by_value]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block Ranging Algorithm - Step 1 , 2 , 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute subgraph induced by C_0. For each node compute the sum of the weigths of the in-edges.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T10:04:17.998718Z",
     "start_time": "2018-12-23T10:04:17.992733Z"
    }
   },
   "outputs": [],
   "source": [
    "C0 = categories[10839]\n",
    "induced_graph = DG.subgraph(C0) # create sub graph for only category zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T10:04:24.285758Z",
     "start_time": "2018-12-23T10:04:24.277783Z"
    }
   },
   "outputs": [],
   "source": [
    "def C0_sum_weights_inedges(induced_graph):\n",
    "    # Iterate to get sum of weights of in-edges\n",
    "    all_weights = {} # it will look like {node1:sum_of_weights, node2:sum_of_weights, ...}\n",
    "    for (node1,node2,data) in induced_graph.edges(data=True):\n",
    "        if node2 not in all_weights.keys(): # we consider node2 because we're checking how many \"incoming\" neighboors\n",
    "            all_weights[node2] = data['weight'] # if node2 doesn't exist in all_weights, just add initial weight\n",
    "        else:\n",
    "            all_weights[node2] += data['weight'] # if node2 already exists in all_weights, add weight up like cumulate weight\n",
    "    \n",
    "    # if there is no incoming neigboors, detect these nodes and give their values as zero\n",
    "    for zero_node in list(set(induced_graph.nodes()) - set(all_weights.keys())):\n",
    "        all_weights[zero_node]=0\n",
    "        \n",
    "    all_weights = sorted(all_weights.items(), key=lambda kv: kv[1], reverse=True) # sort with descending order by values in dictionary \n",
    "\n",
    "    return all_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T10:04:30.760171Z",
     "start_time": "2018-12-23T10:04:30.646535Z"
    }
   },
   "outputs": [],
   "source": [
    "C0_score = C0_sum_weights_inedges(induced_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "\n",
    "Extending the graph to the nodes that belong to $C_1$. Thus, for each article in $C_1$ compute the score as before. Note that the in-edges coming from the previous category, $C_0$, have as weights the score of the node that sends the edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T10:04:37.307869Z",
     "start_time": "2018-12-23T10:04:37.299891Z"
    }
   },
   "outputs": [],
   "source": [
    "C1 = categories[sorted_by_value[0][0]]\n",
    "sub_graph = DG.subgraph(list(C0) + C1) # create sub graph for only category 0 and category 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T10:04:45.810631Z",
     "start_time": "2018-12-23T10:04:43.556886Z"
    }
   },
   "outputs": [],
   "source": [
    "C1_score = {} # it is the score for only category 1\n",
    "\n",
    "\"\"\" apply same steps as step 1. The only difference is we use C0 as induced category. That means Since C0 is the first \n",
    "    ordered category, we need to take into account if there incoming arrows(directions) from catefory 0. We have to count\n",
    "    also. So thats why we create sub graph with C0 and C1\"\"\"\n",
    "for (node1,node2,data) in sub_graph.edges(data=True):\n",
    "    if node2 in C1:\n",
    "        if node2 not in C1_score.keys():\n",
    "            C1_score[node2] = data['weight']\n",
    "        else:\n",
    "            C1_score[node2] += data['weight']\n",
    "            \n",
    "for zero_node in list(set(C1) - set(C1_score.keys())):\n",
    "    C1_score[zero_node]=0\n",
    "        \n",
    "C1_score = sorted(C1_score.items(), key=lambda kv: kv[1], reverse=True) # sort by values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3\n",
    "\n",
    "Repeating Step2 up to the last category of the ranking. In the last step of the example you clearly see the weight update of the edge coming from node E."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T10:05:38.161271Z",
     "start_time": "2018-12-23T10:05:38.154287Z"
    }
   },
   "outputs": [],
   "source": [
    "def score(DG, sorted_categories_idx, C0_score):\n",
    "    C_0 = list(categories[sorted_categories_idx[0]])\n",
    "    cum_nodes_list = C_0 # cumulative nodes list\n",
    "    all_weights = {}\n",
    "    all_weights.update(C0_score) # append induced category rank and sum of weights\n",
    "\n",
    "    for cat_idx in tqdm_notebook(sorted_categories_idx):\n",
    "        C_i = list(categories[cat_idx])\n",
    "        cum_nodes_list = cum_nodes_list + C_i # we need to build sub graph cumulatively\n",
    "        sub_graph = DG.subgraph(cum_nodes_list)\n",
    "        \n",
    "        cat_weights = {} # weights for only category C_i\n",
    "        for (node1,node2,data) in sub_graph.edges(data=True):\n",
    "            if node2 in C_i:\n",
    "                if node2 not in cat_weights.keys():\n",
    "                    cat_weights[node2] = data['weight']\n",
    "                else:\n",
    "                    cat_weights[node2] += data['weight']\n",
    "    \n",
    "        for zero_node in list(set(C_i) - set(cat_weights.keys())):\n",
    "            cat_weights[zero_node] = 0\n",
    "            \n",
    "        cat_weights = sorted(cat_weights.items(), key=lambda kv: kv[1], reverse=True) # before adding to all_weights sort weights just inside the category C_i\n",
    "        all_weights.update(cat_weights) # add cat_weights to all_weights\n",
    "    \n",
    "    return all_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_rank = score(DG, sorted_categories_idx, C0_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T11:53:24.064578Z",
     "start_time": "2018-12-23T11:53:24.001745Z"
    }
   },
   "outputs": [],
   "source": [
    "sorted_by_value = sorted(nodes_rank.items(), key=lambda kv: kv[1])\n",
    "top_10 = sorted_by_value[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T12:03:12.928557Z",
     "start_time": "2018-12-23T12:03:12.919579Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 10 connections with the input category are the articles:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(896828, 1323),\n",
       " (1041937, 1506),\n",
       " (1090186, 1560),\n",
       " (1163290, 1852),\n",
       " (1060414, 1855),\n",
       " (1061904, 2526),\n",
       " (81952, 3367),\n",
       " (1174967, 3580),\n",
       " (870589, 10565),\n",
       " (279122, 18967)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The top 10 connections with the input category are the articles:\")\n",
    "top_10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
